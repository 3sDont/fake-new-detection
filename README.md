# ğŸ•µï¸â€â™‚ï¸ Project PhÃ¡t hiá»‡n Tin tá»©c Giáº£ máº¡o báº±ng cÃ¡c mÃ´ hÃ¬nh Transformer

Dá»± Ã¡n nÃ y táº­p trung vÃ o viá»‡c xÃ¢y dá»±ng vÃ  Ä‘Ã¡nh giÃ¡ má»™t pipeline hoÃ n chá»‰nh Ä‘á»ƒ phÃ¡t hiá»‡n tin tá»©c giáº£ máº¡o (Fake News Detection) báº±ng cÃ¡ch sá»­ dá»¥ng cÃ¡c kiáº¿n trÃºc Transformer. Ba mÃ´ hÃ¬nh Transformer tiÃªu biá»ƒu Ä‘Ã£ Ä‘Æ°á»£c lá»±a chá»n Ä‘á»ƒ so sÃ¡nh hiá»‡u suáº¥t: **BERT**, **RoBERTa**, vÃ  **XLNet**.

---

## ğŸš€ Tá»•ng quan

Trong bá»‘i cáº£nh tin giáº£ ngÃ y cÃ ng lan rá»™ng, viá»‡c xÃ¢y dá»±ng cÃ¡c há»‡ thá»‘ng tá»± Ä‘á»™ng Ä‘á»ƒ xÃ¡c minh tÃ­nh xÃ¡c thá»±c cá»§a thÃ´ng tin lÃ  vÃ´ cÃ¹ng cáº§n thiáº¿t. Äá»“ Ã¡n nÃ y tiáº¿p cáº­n bÃ i toÃ¡n báº±ng cÃ¡ch á»©ng dá»¥ng sá»©c máº¡nh cá»§a cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (Large Language Models), cá»¥ thá»ƒ lÃ  cÃ¡c kiáº¿n trÃºc Transformer.

Há»‡ thá»‘ng Ä‘Æ°á»£c thiáº¿t káº¿ theo hÆ°á»›ng module hÃ³a, bao gá»“m Ä‘áº§y Ä‘á»§ cÃ¡c giai Ä‘oáº¡n cá»§a má»™t dá»± Ã¡n Khoa há»c Dá»¯ liá»‡u:
1.  **Táº£i vÃ  khÃ¡m phÃ¡ dá»¯ liá»‡u (EDA)**
2.  **Tiá»n xá»­ lÃ½ vÃ  lÃ m sáº¡ch dá»¯ liá»‡u**
3.  **Tokenization vÃ  chuáº©n bá»‹ dá»¯ liá»‡u**
4.  **Huáº¥n luyá»‡n vÃ  tinh chá»‰nh (Fine-tuning) mÃ´ hÃ¬nh**
5.  **ÄÃ¡nh giÃ¡ hiá»‡u suáº¥t vÃ  so sÃ¡nh**
6.  **Dá»± Ä‘oÃ¡n trÃªn dá»¯ liá»‡u má»›i (Inference)**

## ğŸ¯ Má»¥c tiÃªu

*   XÃ¢y dá»±ng má»™t pipeline NLP hoÃ n chá»‰nh, cÃ³ kháº£ nÄƒng tÃ¡i sá»­ dá»¥ng vÃ  má»Ÿ rá»™ng.
*   á»¨ng dá»¥ng vÃ  so sÃ¡nh hiá»‡u suáº¥t cá»§a ba mÃ´ hÃ¬nh Transformer phá»• biáº¿n (BERT, RoBERTa, XLNet).
*   ÄÃ¡nh giÃ¡ toÃ n diá»‡n cÃ¡c mÃ´ hÃ¬nh dá»±a trÃªn cÃ¡c chá»‰ sá»‘ tiÃªu chuáº©n (Accuracy, Precision, Recall, F1-score).
*   Cung cáº¥p má»™t há»‡ thá»‘ng cÃ³ kháº£ nÄƒng dá»± Ä‘oÃ¡n nhÃ£n (Tháº­t/Giáº£) cho má»™t vÄƒn báº£n tin tá»©c má»›i.

## ğŸ“Š Dá»¯ liá»‡u vÃ  PhÃ¢n tÃ­ch KhÃ¡m phÃ¡ (EDA)

*   **Nguá»“n dá»¯ liá»‡u**: Bá»™ dá»¯ liá»‡u Ä‘Æ°á»£c láº¥y tá»« Kaggle: [Fake News Dataset](https://www.kaggle.com/datasets/basdong/fake-news-dataset).
*   **KÃ­ch thÆ°á»›c**: ~78,617 máº«u tin tá»©c.
*   **PhÃ¢n phá»‘i nhÃ£n**: Dá»¯ liá»‡u khÃ¡ cÃ¢n báº±ng (44.5% tin tháº­t, 55.5% tin giáº£), lÃ  Ä‘iá»u kiá»‡n lÃ½ tÆ°á»Ÿng Ä‘á»ƒ huáº¥n luyá»‡n mÃ´ hÃ¬nh.

#### PhÃ¢n tÃ­ch tá»« khÃ³a ná»•i báº­t

PhÃ¢n tÃ­ch Word Cloud vÃ  N-grams cho tháº¥y sá»± khÃ¡c biá»‡t rÃµ rá»‡t giá»¯a hai loáº¡i tin:
*   **Tin giáº£**: ThÆ°á»ng táº­p trung vÃ o cÃ¡c chá»§ Ä‘á» chÃ­nh trá»‹ gÃ¢y tranh cÃ£i vÃ  cÃ¡c nhÃ¢n váº­t ná»•i tiáº¿ng nhÆ° "Trump", "Clinton", "Obama".
*   **Tin tháº­t**: Sá»­ dá»¥ng ngÃ´n ngá»¯ chÃ­nh thá»‘ng, mang tÃ­nh thá»ƒ cháº¿ hÆ¡n nhÆ° "government", "state", "Republican".

| Word Cloud Tin Tháº­t | Word Cloud Tin Giáº£ |
| :---: | :---: |
| <img src="./images/image-6.png" width="400"> | <img src="./images/image-7.png" width="400"> |

---

## âš™ï¸ Thiáº¿t káº¿ Pipeline

Há»‡ thá»‘ng Ä‘Æ°á»£c xÃ¢y dá»±ng dá»±a trÃªn cÃ¡c lá»›p (class) Python Ä‘á»™c láº­p, giÃºp mÃ£ nguá»“n trá»Ÿ nÃªn rÃµ rÃ ng vÃ  dá»… báº£o trÃ¬:

| Lá»›p (Class) | Chá»©c nÄƒng |
| :--- | :--- |
| `KagleDataLoader` | Táº£i vÃ  há»£p nháº¥t dá»¯ liá»‡u tá»« KaggleHub. |
| `EDAVisualizer` | PhÃ¢n tÃ­ch vÃ  trá»±c quan hÃ³a dá»¯ liá»‡u. |
| `DataPreprocessor` | LÃ m sáº¡ch vÃ  chuáº©n hÃ³a dá»¯ liá»‡u vÄƒn báº£n. |
| `TextDatasetBuilder` | Tokenize vÃ  táº¡o `DatasetDict` cho Hugging Face. |
| `ModelBuilder` | Táº£i vÃ  cáº¥u hÃ¬nh mÃ´ hÃ¬nh Transformer. |
| `ModelTrainer` | ÄÃ³ng gÃ³i vÃ  thá»±c hiá»‡n quÃ¡ trÃ¬nh huáº¥n luyá»‡n báº±ng `Trainer` API. |
| `ModelEvaluator` | ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh trÃªn táº­p kiá»ƒm thá»­. |
| `FakeNewsPipelineManager` | Äiá»u phá»‘i toÃ n bá»™ luá»“ng cÃ´ng viá»‡c. |

## ğŸ“ˆ Káº¿t quáº£ Huáº¥n luyá»‡n vÃ  ÄÃ¡nh giÃ¡

CÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n vá»›i cÃ¡c siÃªu tham sá»‘ tá»‘i Æ°u, bao gá»“m cáº£ ká»¹ thuáº­t **Mixed-Precision (FP16)** Ä‘á»ƒ tÄƒng tá»‘c vÃ  **Early Stopping** Ä‘á»ƒ chá»‘ng overfitting.

#### Báº£ng tá»•ng há»£p hiá»‡u suáº¥t

| MÃ´ hÃ¬nh | Accuracy | Precision | Recall | F1-Score |
| :--- | :---: | :---: | :---: | :---: |
| âœ… **RoBERTa (roberta-base)** | 99.83% | 99.80% | 99.83% | **99.81%** |
| **BERT (bert-base-uncased)** | 99.35% | 99.45% | 99.13% | **99.18%** |
| **XLNet (xlnet-base-cased)** | 98.51% | 98.87% | 97.77% | **98.32%** |

#### Ma tráº­n nháº§m láº«n (Confusion Matrix)

| RoBERTa | BERT | XLNet |
| :---: | :---: | :---: |
| <img src="./images/image-10.png" width="300"> | <img src="./images/image-9.png" width="300"> | <img src="./images/image-11.png" width="300"> |

**Tháº£o luáº­n káº¿t quáº£:**
*   **RoBERTa** lÃ  mÃ´ hÃ¬nh cÃ³ hiá»‡u suáº¥t vÆ°á»£t trá»™i nháº¥t, cho tháº¥y kháº£ nÄƒng náº¯m báº¯t ngá»¯ cáº£nh sÃ¢u sáº¯c vÃ  phÃ¢n biá»‡t cÃ¡c sáº¯c thÃ¡i ngÃ´n ngá»¯ tinh vi.
*   **BERT** cÅ©ng thá»ƒ hiá»‡n sá»©c máº¡nh áº¥n tÆ°á»£ng vÃ  lÃ  má»™t lá»±a chá»n cÃ¢n báº±ng giá»¯a hiá»‡u suáº¥t vÃ  tÃ i nguyÃªn tÃ­nh toÃ¡n.
*   **XLNet** cho káº¿t quáº£ tá»‘t á»Ÿ epoch Ä‘áº§u tiÃªn nhÆ°ng nhanh chÃ³ng bá»‹ overfitting, cho tháº¥y mÃ´ hÃ¬nh nÃ y cáº§n cÃ¡c ká»¹ thuáº­t Ä‘iá»u chuáº©n máº¡nh hÆ¡n trÃªn bá»™ dá»¯ liá»‡u nÃ y.

## ğŸ”® Káº¿t quáº£ Dá»± Ä‘oÃ¡n trÃªn Dá»¯ liá»‡u Má»›i

Há»‡ thá»‘ng Ä‘Æ°á»£c thá»­ nghiá»‡m vá»›i má»™t sá»‘ tiÃªu Ä‘á» tin tá»©c giáº­t gÃ¢n vÃ  Ä‘Ã£ Ä‘Æ°a ra dá»± Ä‘oÃ¡n chÃ­nh xÃ¡c:

| TiÃªu Ä‘á» Tin tá»©c | NhÃ£n Dá»± Ä‘oÃ¡n |
| :--- | :---: |
| "NASA confirms presence of microbial life on Europa." | **FAKE** |
| "Bill Gates to implant tracking chips via vaccines, claims viral post." | **FAKE** |
| "Biden signs executive order on AI regulation." | **FAKE** |
| "Apple releases new iPhone model with brainwave control." | **FAKE**|
| "WHO warns of new COVID variant emerging in Southeast Asia." | **FAKE** |

Káº¿t quáº£ nÃ y cho tháº¥y cÃ¡c mÃ´ hÃ¬nh Ä‘Ã£ há»c Ä‘Æ°á»£c cÃ¡c Ä‘áº·c trÆ°ng cá»‘t lÃµi cá»§a tin giáº£ vÃ  cÃ³ kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a tá»‘t.

## ğŸ’¡ Káº¿t luáº­n vÃ  HÆ°á»›ng phÃ¡t triá»ƒn

*   **Káº¿t luáº­n**: Äá»“ Ã¡n Ä‘Ã£ xÃ¢y dá»±ng thÃ nh cÃ´ng má»™t pipeline NLP hiá»‡u quáº£. **RoBERTa** Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh lÃ  mÃ´ hÃ¬nh tá»‘t nháº¥t cho bÃ i toÃ¡n nÃ y, trong khi **BERT** lÃ  má»™t lá»±a chá»n thay tháº¿ thá»±c táº¿ vÃ  máº¡nh máº½.
*   **HÆ°á»›ng phÃ¡t triá»ƒn**:
    *   **Má»Ÿ rá»™ng mÃ´ hÃ¬nh**: Thá»­ nghiá»‡m vá»›i cÃ¡c mÃ´ hÃ¬nh lá»›n hÆ¡n vÃ  tiÃªn tiáº¿n hÆ¡n (`DeBERTa`, `Longformer`).
    *   **Cáº£i thiá»‡n dá»¯ liá»‡u**: TÄƒng cÆ°á»ng dá»¯ liá»‡u Ä‘a dáº¡ng hÆ¡n vÃ  má»Ÿ rá»™ng sang cÃ¡c ngÃ´n ngá»¯ khÃ¡c (vÃ­ dá»¥: tiáº¿ng Viá»‡t).
    *   **Triá»ƒn khai**: Tá»‘i Æ°u hÃ³a mÃ´ hÃ¬nh (quantization, pruning) vÃ  triá»ƒn khai dÆ°á»›i dáº¡ng má»™t á»©ng dá»¥ng web/API.
    *   **TÄƒng tÃ­nh giáº£i thÃ­ch**: Sá»­ dá»¥ng cÃ¡c cÃ´ng cá»¥ nhÆ° LIME/SHAP Ä‘á»ƒ giáº£i thÃ­ch lÃ½ do táº¡i sao má»™t tin tá»©c Ä‘Æ°á»£c phÃ¢n loáº¡i lÃ  tháº­t/giáº£.

---

## ğŸ”§ CÃ¡ch cháº¡y Project

1.  **Clone repository nÃ y vá» mÃ¡y cá»§a báº¡n.**
2.  **CÃ i Ä‘áº·t cÃ¡c thÆ° viá»‡n cáº§n thiáº¿t:**
    ```bash
    pip install -r requirements.txt
    ```
3.  **Chuáº©n bá»‹ API Keys:**
    Äá»ƒ theo dÃµi quÃ¡ trÃ¬nh huáº¥n luyá»‡n, báº¡n cáº§n cÃ³ tÃ i khoáº£n [Hugging Face](https://huggingface.co/) vÃ  [Weights & Biases](https://wandb.ai/). Sau Ä‘Ã³, thiáº¿t láº­p cÃ¡c biáº¿n mÃ´i trÆ°á»ng sau:
    ```bash
    export HUGGINGFACE_TOKEN='your_hf_token'
    export WANDB_API_KEY='your_wandb_key'
    ```
4.  **Cháº¡y Notebook:**
    Má»Ÿ vÃ  cháº¡y file `BÃ¡o-cÃ¡o-NLP.ipynb` trong mÃ´i trÆ°á»ng Jupyter hoáº·c Google Colab (khuyáº¿n khÃ­ch sá»­ dá»¥ng GPU Ä‘á»ƒ tÄƒng tá»‘c Ä‘á»™ huáº¥n luyá»‡n).
